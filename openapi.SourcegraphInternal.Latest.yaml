openapi: 3.0.0
info:
  title: Sourcegraph Internal API
  version: Latest
tags: []
paths:
  /.api/client-config:
    get:
      operationId: getClientConfig
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClientConfig'
  /.api/completions/code:
    post:
      operationId: Completions_completions
      summary: Get LLM completions as a non-streaming response
      parameters:
        - name: api-version
          in: query
          required: false
          schema:
            $ref: '#/components/schemas/CompletionRequestApiVersion'
        - name: client-name
          in: query
          required: false
          schema:
            type: string
        - name: client-version
          in: query
          required: false
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionRequest'
  /.api/completions/stream:
    post:
      operationId: Completions_chatCompletions
      summary: Get streaming LLM completions with SSE
      parameters:
        - name: api-version
          in: query
          required: false
          schema:
            $ref: '#/components/schemas/CompletionRequestApiVersion'
        - name: client-name
          in: query
          required: false
          schema:
            type: string
        - name: client-version
          in: query
          required: false
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionRequest'
  /.api/modelconfig/supported-models.json:
    get:
      operationId: getSupportedModels
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelCatalog'
components:
  schemas:
    ClientConfig:
      type: object
      required:
        - codyEnabled
        - chatEnabled
        - autoCompleteEnabled
        - customCommandsEnabled
        - attributionEnabled
        - smartContextWindowEnabled
        - modelsAPIEnabled
      properties:
        codyEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of Cody at all.
        chatEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody chat feature.
        autoCompleteEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody autocomplete feature.
        customCommandsEnabled:
          type: boolean
          description: Whether the site admin allows the user to make use of the **custom** Cody commands feature.
        attributionEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody attribution feature.
        smartContextWindowEnabled:
          type: boolean
          description: |-
            Whether the 'smart context window' feature should be enabled, and whether the Sourcegraph
            instance supports various new GraphQL APIs needed to make it work.
        modelsAPIEnabled:
          type: boolean
          description: |-
            Whether the new Sourcegraph backend LLM models API endpoint should be used to query which
            models are available.
        latestSupportedCompletionsStreamAPIVersion:
          type: integer
          format: int32
          description: |-
            The latest `api-version=N` query parameter that is supported by the
            `POST /.api/completions/stream` endpoint. This field was added after
            `api-version=2` so you will never get a response with `api-version=1` or
            lower even when `api-version=1` is supported on the instance.
      description: |-
        This is the JSON object which all clients request after authentication to determine how
        they should behave, e.g. if a site admin has restricted chat/autocomplete/other functionality,
        if experimental features are available, etc.

        The configuration is always specific to a single authenticated user.

        Adding new fields here is fine, but you cannot make backwards-incompatible changes (removing
        fields or change the meaning of fields in backwards-incompatible ways.) If you need to do that,
        then read up on https://github.com/sourcegraph/sourcegraph/pull/63591#discussion_r1663211601

        After adding a field here, you can implement it in cmd/frontend/internal/clientconfig/clientconfig.go
        GetForActor method.
    CompletionMessage:
      type: object
      required:
        - speaker
        - text
      properties:
        speaker:
          type: string
          description: |-
            The speaker of the message.

            For historical reasons, this API uses Anthropic naming conventions.  The
            valid values are:

            - "human" (Anthropic) instead of "role: user" (OpenAI).
            - "assistant", same as OpenAI's "role: assistant"
            - "system", same as OpenAI's "role: system"
        text:
          type: string
          description: The text of the message.
    CompletionRequest:
      type: object
      required:
        - prompt
        - model
        - messages
      properties:
        prompt:
          type: string
        model:
          type: string
        messages:
          type: array
          items:
            $ref: '#/components/schemas/CompletionMessage'
        maxTokensToSample:
          type: integer
          format: int32
        temperature:
          type: number
          format: float
        stopSequences:
          type: array
          items:
            type: string
        topK:
          type: integer
          format: int32
        topP:
          type: number
          format: float
        stream:
          type: boolean
        logprobs:
          type: integer
          format: uint8
    CompletionRequestApiVersion:
      type: number
      enum:
        - 1
        - 2
    CompletionResponse:
      type: object
      properties:
        completion:
          type: string
          description: |-
            Completion is the full completion string. This field
            is only present when using the `/.api/completions/code` or when
            using `api-version=1` or older with `/.api/completions/stream`.

            In the V2 API for streaming responses, the `deltaText` property is used
            instead.
        deltaText:
          type: string
          description: |-
            DeltaText is the incremental text that was added to the prompt.
            This field is only present in streaming responses to `/.api/completions/stream`.
        stopReason:
          type: string
          description: |-
            The reason the model stopped generating tokens. The exact format
            of this field is defined by the model provider.

            For OpenAI models, the following stop reasons are defined:
            - `stop`: the model hit a natural stop point or a provided stop sequence.
            - `length`: the maximum number of tokens specified in the request was reached.
            - `content_filter`: content was omitted due content filters.

            For Anthropic models, the following stop reasons are defined:
            - "stop_sequence": we reached a stop sequence either provided via the
            stop_sequences parameter, or a stop sequence built into the model.
            - "max_tokens": we exceeded max_tokens_to_sample or the model's maximum limit.
        logprobs:
          type: object
          allOf:
            - $ref: '#/components/schemas/Logprobs'
          nullable: true
    ContextWindow:
      type: object
      required:
        - maxInputTokens
        - maxOutputTokens
      properties:
        maxInputTokens:
          type: integer
          format: int32
        maxOutputTokens:
          type: integer
          format: int32
    DefaultModels:
      type: object
      required:
        - chat
        - fastChat
        - codeCompletion
      properties:
        chat:
          type: string
        fastChat:
          type: string
        codeCompletion:
          type: string
    Logprobs:
      type: object
      required:
        - tokens
        - token_logprobs
        - top_logprobs
        - text_offset
      properties:
        tokens:
          type: array
          items:
            type: string
        token_logprobs:
          type: array
          items:
            type: number
            format: float
        top_logprobs:
          type: array
          items:
            type: object
            additionalProperties:
              type: number
              format: float
        text_offset:
          type: array
          items:
            type: integer
            format: int32
    Model:
      type: object
      required:
        - modelRef
        - displayName
        - modelName
        - capabilities
        - category
        - status
        - tier
        - contextWindow
      properties:
        modelRef:
          type: string
        displayName:
          type: string
        modelName:
          type: string
        capabilities:
          type: array
          items:
            $ref: '#/components/schemas/ModelCapability'
        category:
          $ref: '#/components/schemas/ModelCategory'
        status:
          $ref: '#/components/schemas/ModelStatus'
        tier:
          $ref: '#/components/schemas/ModelTier'
        contextWindow:
          $ref: '#/components/schemas/ContextWindow'
    ModelCapability:
      type: string
      enum:
        - autocomplete
        - chat
    ModelCatalog:
      type: object
      required:
        - schemaVersion
        - revision
        - providers
        - models
        - defaultModels
      properties:
        schemaVersion:
          type: string
        revision:
          type: string
        providers:
          type: array
          items:
            $ref: '#/components/schemas/Provider'
        models:
          type: array
          items:
            $ref: '#/components/schemas/Model'
        defaultModels:
          $ref: '#/components/schemas/DefaultModels'
    ModelCategory:
      type: string
      enum:
        - accuracy
        - balanced
        - speed
    ModelStatus:
      type: string
      enum:
        - experimental
        - beta
        - stable
        - deprecated
    ModelTier:
      type: string
      enum:
        - free
        - pro
        - enterprise
    Provider:
      type: object
      required:
        - id
        - displayName
      properties:
        id:
          type: string
        displayName:
          type: string
    Versions:
      type: string
      enum:
        - V5_5
        - V5_6
        - V5_7
        - V5_8
        - Latest
