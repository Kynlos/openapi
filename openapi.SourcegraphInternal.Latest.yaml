openapi: 3.0.0
info:
  title: Sourcegraph Internal API
  version: Latest
tags: []
paths:
  /.api/client-config:
    get:
      operationId: getClientConfig
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClientConfig'
  /.api/completions/code:
    post:
      operationId: Completions_completions
      summary: Get LLM completions as a non-streaming response
      parameters:
        - name: api-version
          in: query
          required: false
          schema:
            $ref: '#/components/schemas/CompletionRequestApiVersion'
        - name: client-name
          in: query
          required: false
          description: The name of the application sending this request. For example, "cody-web" or "jetbrains".
          schema:
            type: string
        - name: client-version
          in: query
          required: false
          description: The version of the application sending this request. For example, "2.0.0".
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                anyOf:
                  - $ref: '#/components/schemas/CompletionResponse'
                  - $ref: '#/components/schemas/CompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionRequest'
  /.api/completions/stream:
    post:
      operationId: Completions_chatCompletions
      summary: Get streaming LLM completions with SSE
      parameters:
        - name: api-version
          in: query
          required: false
          schema:
            $ref: '#/components/schemas/CompletionRequestApiVersion'
        - name: client-name
          in: query
          required: false
          description: The name of the application sending this request. For example, "cody-web" or "jetbrains".
          schema:
            type: string
        - name: client-version
          in: query
          required: false
          description: The version of the application sending this request. For example, "2.0.0".
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                anyOf:
                  - $ref: '#/components/schemas/CompletionResponse'
                  - $ref: '#/components/schemas/CompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionRequest'
  /.api/modelconfig/supported-models.json:
    get:
      operationId: getSupportedModels
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelCatalog'
security:
  - SourcegraphTokenAuth: []
components:
  schemas:
    ClientConfig:
      type: object
      required:
        - codyEnabled
        - chatEnabled
        - autoCompleteEnabled
        - customCommandsEnabled
        - attributionEnabled
        - smartContextWindowEnabled
        - modelsAPIEnabled
      properties:
        codyEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of Cody at all.
        chatEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody chat feature.
        autoCompleteEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody autocomplete feature.
        customCommandsEnabled:
          type: boolean
          description: Whether the site admin allows the user to make use of the **custom** Cody commands feature.
        attributionEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody attribution feature.
        smartContextWindowEnabled:
          type: boolean
          description: |-
            Whether the 'smart context window' feature should be enabled, and whether the Sourcegraph
            instance supports various new GraphQL APIs needed to make it work.
        modelsAPIEnabled:
          type: boolean
          description: |-
            Whether the new Sourcegraph backend LLM models API endpoint should be used to query which
            models are available.
        latestSupportedCompletionsStreamAPIVersion:
          type: integer
          format: int32
          description: |-
            The latest `api-version=N` query parameter that is supported by the
            `POST /.api/completions/stream` endpoint. This field was added after
            `api-version=2` so you will never get a response with `api-version=1` or
            lower even when `api-version=1` is supported on the instance.
      description: |-
        This is the JSON object which all clients request after authentication to determine how
        they should behave, e.g. if a site admin has restricted chat/autocomplete/other functionality,
        if experimental features are available, etc.

        The configuration is always specific to a single authenticated user.

        Adding new fields here is fine, but you cannot make backwards-incompatible changes (removing
        fields or change the meaning of fields in backwards-incompatible ways.) If you need to do that,
        then read up on https://github.com/sourcegraph/sourcegraph/pull/63591#discussion_r1663211601

        After adding a field here, you can implement it in cmd/frontend/internal/clientconfig/clientconfig.go
        GetForActor method.
    ClientSideModelConfig:
      type: object
      properties:
        openAICompatible:
          type: object
          allOf:
            - $ref: '#/components/schemas/ClientSideModelConfigOpenAICompatible'
          nullable: true
      description: |-
        Anything that needs to be provided to Cody clients at the model-level can go here.

        For example, allowing the server to customize/override the LLM
        prompt used. Or describe how clients should upload context to
        remote servers, etc. Or "hints", like "this model is great when
        working with 'C' code.".
    ClientSideModelConfigOpenAICompatible:
      type: object
      properties:
        stopSequences:
          type: array
          items:
            type: string
          description: (optional) List of stop sequences to use for this model.
        endOfText:
          type: string
          description: (optional) EndOfText identifier used by the model. e.g. "<|endoftext|>", "< EOT >"
        contextSizeHintTotalCharacters:
          type: integer
          format: uint32
          nullable: true
          minimum: 0
          description: |-
            (optional) A hint the client should use when producing context to send to the LLM.
            The maximum length of all context (prefix + suffix + snippets), in characters.
        contextSizeHintPrefixCharacters:
          type: integer
          format: uint32
          nullable: true
          minimum: 0
          description: |-
            (optional) A hint the client should use when producing context to send to the LLM.
            The maximum length of the document prefix (text before the cursor) to include, in characters.
        contextSizeHintSuffixCharacters:
          type: integer
          format: uint32
          nullable: true
          minimum: 0
          description: |-
            (optional) A hint the client should use when producing context to send to the LLM.
            The maximum length of the document suffix (text after the cursor) to include, in characters.
        chatPreInstruction:
          type: string
          description: |-
            (optional) Custom instruction to be included at the start of all chat messages
            when using this model, e.g. "Answer all questions in Spanish."

            Note: similar to Cody client config option `cody.chat.preInstruction`; if user has
            configured that it will be used instead of this.
        editPostInstruction:
          type: string
          description: |-
            (optional) Custom instruction to be included at the end of all edit commands
            when using this model, e.g. "Write all unit tests with Jest instead of detected framework."

            Note: similar to Cody client config option `cody.edit.preInstruction`; if user has
            configured that it will be respected instead of this.
        autocompleteSinglelineTimeout:
          type: integer
          format: uint32
          minimum: 0
          description: |-
            (optional) How long the client should wait for autocomplete results to come back (milliseconds),
            before giving up and not displaying an autocomplete result at all.

            This applies on single-line completions, e.g. `var i = <completion>`

            Note: similar to hidden Cody client config option `cody.autocomplete.advanced.timeout.singleline`
            If user has configured that, it will be respected instead of this.
        autocompleteMultilineTimeout:
          type: integer
          format: uint32
          minimum: 0
          description: |-
            (optional) How long the client should wait for autocomplete results to come back (milliseconds),
            before giving up and not displaying an autocomplete result at all.

            This applies on multi-line completions, which are based on intent-detection when e.g. a code block
            is being completed, e.g. `func parseURL(url string) {<completion>`

            Note: similar to hidden Cody client config option `cody.autocomplete.advanced.timeout.multiline`
            If user has configured that, it will be respected instead of this.
        chatTopK:
          type: number
          format: double
          nullable: true
          description: (optional) model parameters to use for the chat feature
        chatTopP:
          type: number
          format: double
          nullable: true
        chatTemperature:
          type: number
          format: double
          nullable: true
          minimum: 0
          maximum: 2
        chatMaxTokens:
          type: integer
          format: uint32
        autoCompleteTopK:
          type: number
          format: double
          nullable: true
          description: (optional) model parameters to use for the autocomplete feature
        autoCompleteTopP:
          type: number
          format: double
          nullable: true
        autoCompleteTemperature:
          type: number
          format: double
          nullable: true
          minimum: 0
          maximum: 2
        autoCompleteSinglelineMaxTokens:
          type: integer
          format: uint32
          minimum: 0
        autoCompleteMultilineMaxTokens:
          type: integer
          format: uint32
          minimum: 0
        editTopK:
          type: number
          format: double
          nullable: true
          description: (optional) model parameters to use for the edit feature
        editTopP:
          type: number
          format: double
          nullable: true
        editTemperature:
          type: number
          format: double
          nullable: true
          minimum: 0
          maximum: 2
        editMaxTokens:
          type: integer
          format: uint32
          minimum: 0
      description: |-
        Client-side model configuration used when the model is backed by an OpenAI-compatible API
        provider.
    CompletionMessage:
      type: object
      required:
        - role
        - content
      properties:
        role:
          type: string
          enum:
            - user
            - assistant
            - system
        content:
          anyOf:
            - type: string
            - type: array
              items:
                $ref: '#/components/schemas/MessageContentPart'
    CompletionRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: |-
            The language model to use for the completion.
            Call `/.api/modelconfig/supported-models.json` to get the list of available models.
        messages:
          type: array
          items:
            $ref: '#/components/schemas/CompletionMessage'
          description: |-
            The messages to generate completions for.
            When using `/.api/completions/code`, only include one message with the speaker 'human'.
        maxTokensToSample:
          type: integer
          format: int32
          description: |-
            The maximum number of tokens to generate before stopping.
            Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
        temperature:
          type: number
          format: double
          description: |-
            Amount of randomness injected into the response.

            Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0
            for analytical / multiple choice, and closer to 1.0 for creative and
            generative tasks.

            Note that even with temperature of 0.0, the results will not be fully
            deterministic.
        stopSequences:
          type: array
          items:
            type: string
          description: |-
            Sequences that will cause the model to stop generating.

            Our models stop on "\n\nHuman:", and may include additional built-in stop
            sequences in the future. By providing the `stop_sequences` parameter, you may
            include additional strings that will cause the model to stop generating.
        topK:
          type: integer
          format: int32
          description: |-
            Only sample from the top K options for each subsequent token.

            Used to remove "long tail" low probability responses. Learn more technical details here:
            https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277

            Recommended for advanced use cases only. You usually only need to use temperature.
        topP:
          type: number
          format: double
          description: |-
            Use nucleus sampling.

            In nucleus sampling, we compute the cumulative distribution over all the
            options for each subsequent token in decreasing probability order and cut it
            off once it reaches a particular probability specified by `top_p`. You should
            either alter temperature or `top_p`, but not both.

            Recommended for advanced use cases only. You usually only need to use temperature.
        stream:
          type: boolean
          nullable: true
          description: |-
            Whether to stream the response.

            - When true, the response body is an SSE stream of `event: completion\ndata: JSON` where
            each JSON blob is a `CompletionResponse`.
            - When false, the response body is a single JSON-encoded `CompletionResponse`.
        logprobs:
          type: integer
          format: uint8
          description: |-
            Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
            The maximum value for logprobs is 5.
            For more details, see Fireworks API docs https://docs.fireworks.ai/api-reference/post-completions
    CompletionRequestApiVersion:
      type: number
      enum:
        - 1
        - 2
        - 3
    CompletionResponse:
      type: object
      properties:
        completion:
          type: string
          description: |-
            Completion is the full completion string. This field
            is only present when using the `/.api/completions/code` or when
            using `api-version=1` or older with `/.api/completions/stream`.

            In the V2 API for streaming responses, the `deltaText` property is used
            instead.
        deltaText:
          type: string
          description: |-
            DeltaText is the incremental text that was added to the prompt.
            This field is only present in streaming responses to `/.api/completions/stream`.
        stopReason:
          type: string
          description: |-
            The reason the model stopped generating tokens. The exact format
            of this field is defined by the model provider.

            For OpenAI models, the following stop reasons are defined:
            - `stop`: the model hit a natural stop point or a provided stop sequence.
            - `length`: the maximum number of tokens specified in the request was reached.
            - `content_filter`: content was omitted due content filters.

            For Anthropic models, the following stop reasons are defined:
            - "stop_sequence": we reached a stop sequence either provided via the
            stop_sequences parameter, or a stop sequence built into the model.
            - "max_tokens": we exceeded max_tokens_to_sample or the model's maximum limit.
        logprobs:
          type: object
          allOf:
            - $ref: '#/components/schemas/Logprobs'
          nullable: true
    ContextWindow:
      type: object
      required:
        - maxInputTokens
        - maxOutputTokens
      properties:
        maxInputTokens:
          type: integer
          format: int32
        maxOutputTokens:
          type: integer
          format: int32
    DefaultModels:
      type: object
      required:
        - chat
        - fastChat
        - codeCompletion
      properties:
        chat:
          type: string
        fastChat:
          type: string
        codeCompletion:
          type: string
    FilePart:
      type: object
      required:
        - type
        - file
      properties:
        type:
          type: string
          enum:
            - file
        file:
          type: object
          properties:
            uri:
              type: string
            language_id:
              type: string
            content:
              type: string
          required:
            - uri
      allOf:
        - $ref: '#/components/schemas/MessageContentPart'
    ImagePart:
      type: object
      required:
        - type
        - image_url
      properties:
        type:
          type: string
          enum:
            - image_url
        image_url:
          type: object
          properties:
            url:
              type: string
            detail:
              type: string
              enum:
                - low
                - high
                - auto
          required:
            - url
      allOf:
        - $ref: '#/components/schemas/MessageContentPart'
    Logprobs:
      type: object
      required:
        - tokens
        - token_logprobs
        - top_logprobs
        - text_offset
      properties:
        tokens:
          type: array
          items:
            type: string
        token_logprobs:
          type: array
          items:
            type: number
            format: double
        top_logprobs:
          type: array
          items:
            type: object
            additionalProperties:
              type: number
              format: double
        text_offset:
          type: array
          items:
            type: integer
            format: int32
    MessageContentPart:
      type: object
      required:
        - type
      properties:
        type:
          type: string
      discriminator:
        propertyName: type
        mapping:
          text: '#/components/schemas/TextPart'
          image_url: '#/components/schemas/ImagePart'
          file: '#/components/schemas/FilePart'
          repo: '#/components/schemas/RepoPart'
    Model:
      type: object
      required:
        - modelRef
        - displayName
        - modelName
        - capabilities
        - category
        - status
        - tier
        - contextWindow
      properties:
        modelRef:
          type: string
        displayName:
          type: string
        modelName:
          type: string
        capabilities:
          type: array
          items:
            $ref: '#/components/schemas/ModelCapability'
        category:
          $ref: '#/components/schemas/ModelCategory'
        status:
          $ref: '#/components/schemas/ModelStatus'
        tier:
          $ref: '#/components/schemas/ModelTier'
        contextWindow:
          $ref: '#/components/schemas/ContextWindow'
        clientSideConfig:
          type: object
          allOf:
            - $ref: '#/components/schemas/ClientSideModelConfig'
          nullable: true
    ModelCapability:
      type: string
      enum:
        - autocomplete
        - chat
    ModelCatalog:
      type: object
      required:
        - schemaVersion
        - revision
        - providers
        - models
        - defaultModels
      properties:
        schemaVersion:
          type: string
        revision:
          type: string
        providers:
          type: array
          items:
            $ref: '#/components/schemas/Provider'
        models:
          type: array
          items:
            $ref: '#/components/schemas/Model'
        defaultModels:
          $ref: '#/components/schemas/DefaultModels'
    ModelCategory:
      type: string
      enum:
        - balanced
        - speed
        - other
        - accuracy
    ModelStatus:
      type: string
      enum:
        - experimental
        - beta
        - stable
        - deprecated
    ModelTier:
      type: string
      enum:
        - free
        - pro
        - enterprise
    Provider:
      type: object
      required:
        - id
        - displayName
      properties:
        id:
          type: string
        displayName:
          type: string
    RepoPart:
      type: object
      required:
        - type
        - repo
      properties:
        type:
          type: string
          enum:
            - repo
        repo:
          type: object
          properties:
            id:
              type: string
            name:
              type: string
            filePatterns:
              type: array
              items:
                type: string
      allOf:
        - $ref: '#/components/schemas/MessageContentPart'
    TextPart:
      type: object
      required:
        - type
      properties:
        type:
          type: string
          enum:
            - text
        text:
          type: string
      allOf:
        - $ref: '#/components/schemas/MessageContentPart'
    Versions:
      type: string
      enum:
        - V5_5
        - V5_6
        - V5_7
        - V5_8
        - Latest
  securitySchemes:
    SourcegraphTokenAuth:
      type: apiKey
      in: header
      name: Authorization
      description: |-
        Authenticate to Sourcegraph APIs with the HTTP header "Authorization" using
        the following formatting:

        ```
        Authorization: token TOKEN_VALUE
        ```
        In most cases, a Sourcegraph access token looks like this `sgp_asdadakjaaaaaaabbbbbbssswwwwaaal2131kasdaakkkkkq21asdasaa`.

        In rare cases, you may encounter other kinds of token formats, which are documented in the table below.

        |                  Token Name                  |                                   Description                                    |            Type            |    Regular Expression     |                         |
        | -------------------------------------------- | -------------------------------------------------------------------------------- | -------------------------- | ------------------------- | ----------------------- |
        | Sourcegraph Access Token (v3)                | Token used to access the Sourcegraph GraphQL API                                 | User-generated             | `sgp_(?:[a-fA-F0-9]{16}\|local)_[a-fA-F0-9]{40}` |
        | Sourcegraph Access Token (v2, deprecated)    | Token used to access the Sourcegraph GraphQL API                                 | User-generated             | `sgp_[a-fA-F0-9]{40}`     |                         |
        | Sourcegraph Access Token (v1, deprecated)    | Token used to access the Sourcegraph GraphQL API                                 | User-generated             | `[a-fA-F0-9]{40}`         |                         |
        | Sourcegraph Dotcom User Gateway Access Token | Token used to grant sourcegraph.com users access to Cody                         | Backend (not user-visible) | `sgd_[a-fA-F0-9]{64}`     |                         |
        | Sourcegraph License Key Token                | Token used for product subscriptions, derived from a Sourcegraph license key     | Backend (not user-visible) | `slk_[a-fA-F0-9]{64}`     |                         |
        | Sourcegraph Enterprise subscription (aka "product subscription") Token       | Token used for Enterprise subscriptions, derived from a Sourcegraph license key | Backend (not user-visible) | `sgs_[a-fA-F0-9]{64}`     |                         |
