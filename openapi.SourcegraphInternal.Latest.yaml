openapi: 3.0.0
info:
  title: Sourcegraph Internal API
  version: Latest
tags: []
paths:
  /.api/client-config:
    get:
      operationId: getClientConfig
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ClientConfig'
  /.api/completions/code:
    post:
      operationId: Completions_completions
      summary: Get LLM completions as a non-streaming response
      parameters:
        - name: api-version
          in: query
          required: false
          description: |-
            What API version to use for this request.

            - `api-version=undefined` the original completions API. Don't use this.
            - `api-version=1` added support for system prompts. Uses `'completion'`
            property for streaming responses..
            - `api-version=2` added support for incremental streaming response. Uses
            `'deltaText'` instead of `'completion'` property.
          schema:
            $ref: '#/components/schemas/CompletionRequestApiVersion'
        - name: client-name
          in: query
          required: false
          description: The name of the application sending this request. For example, "cody-web" or "jetbrains".
          schema:
            type: string
        - name: client-version
          in: query
          required: false
          description: The version of the application sending this request. For example, "2.0.0".
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                anyOf:
                  - $ref: '#/components/schemas/CompletionResponse'
                  - $ref: '#/components/schemas/CompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionRequest'
  /.api/completions/stream:
    post:
      operationId: Completions_chatCompletions
      summary: Get streaming LLM completions with SSE
      parameters:
        - name: api-version
          in: query
          required: false
          description: |-
            What API version to use for this request.

            - `api-version=undefined` the original completions API. Don't use this.
            - `api-version=1` added support for system prompts. Uses `'completion'`
            property for streaming responses..
            - `api-version=2` added support for incremental streaming response. Uses
            `'deltaText'` instead of `'completion'` property.
          schema:
            $ref: '#/components/schemas/CompletionRequestApiVersion'
        - name: client-name
          in: query
          required: false
          description: The name of the application sending this request. For example, "cody-web" or "jetbrains".
          schema:
            type: string
        - name: client-version
          in: query
          required: false
          description: The version of the application sending this request. For example, "2.0.0".
          schema:
            type: string
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                anyOf:
                  - $ref: '#/components/schemas/CompletionResponse'
                  - $ref: '#/components/schemas/CompletionResponse'
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CompletionRequest'
  /.api/modelconfig/supported-models.json:
    get:
      operationId: getSupportedModels
      parameters: []
      responses:
        '200':
          description: The request has succeeded.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ModelCatalog'
components:
  schemas:
    ClientConfig:
      type: object
      required:
        - codyEnabled
        - chatEnabled
        - autoCompleteEnabled
        - customCommandsEnabled
        - attributionEnabled
        - smartContextWindowEnabled
        - modelsAPIEnabled
      properties:
        codyEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of Cody at all.
        chatEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody chat feature.
        autoCompleteEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody autocomplete feature.
        customCommandsEnabled:
          type: boolean
          description: Whether the site admin allows the user to make use of the **custom** Cody commands feature.
        attributionEnabled:
          type: boolean
          description: Whether the site admin allows this user to make use of the Cody attribution feature.
        smartContextWindowEnabled:
          type: boolean
          description: |-
            Whether the 'smart context window' feature should be enabled, and whether the Sourcegraph
            instance supports various new GraphQL APIs needed to make it work.
        modelsAPIEnabled:
          type: boolean
          description: |-
            Whether the new Sourcegraph backend LLM models API endpoint should be used to query which
            models are available.
        latestSupportedCompletionsStreamAPIVersion:
          type: integer
          format: int32
          description: |-
            The latest `api-version=N` query parameter that is supported by the
            `POST /.api/completions/stream` endpoint. This field was added after
            `api-version=2` so you will never get a response with `api-version=1` or
            lower even when `api-version=1` is supported on the instance.
      description: |-
        This is the JSON object which all clients request after authentication to determine how
        they should behave, e.g. if a site admin has restricted chat/autocomplete/other functionality,
        if experimental features are available, etc.

        The configuration is always specific to a single authenticated user.

        Adding new fields here is fine, but you cannot make backwards-incompatible changes (removing
        fields or change the meaning of fields in backwards-incompatible ways.) If you need to do that,
        then read up on https://github.com/sourcegraph/sourcegraph/pull/63591#discussion_r1663211601

        After adding a field here, you can implement it in cmd/frontend/internal/clientconfig/clientconfig.go
        GetForActor method.
    CompletionMessage:
      type: object
      required:
        - speaker
        - text
      properties:
        speaker:
          type: string
          description: |-
            The speaker of the message.

            For historical reasons, this API uses Anthropic naming conventions.  The
            valid values are:

            - "human" (Anthropic) instead of "role: user" (OpenAI).
            - "assistant", same as OpenAI's "role: assistant"
            - "system", same as OpenAI's "role: system"
        text:
          type: string
          description: The text of the message.
    CompletionRequest:
      type: object
      required:
        - model
        - messages
      properties:
        model:
          type: string
          description: |-
            The language model to use for the completion.
            Call `/.api/modelconfig/supported-models.json` to get the list of available models.
        messages:
          type: array
          items:
            $ref: '#/components/schemas/CompletionMessage'
          description: |-
            The messages to generate completions for.
            When using `/.api/completions/code`, only include one message with the speaker 'human'.
        maxTokensToSample:
          type: integer
          format: int32
          description: |-
            The maximum number of tokens to generate before stopping.
            Note that our models may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.
        temperature:
          type: number
          format: float
          description: |-
            Amount of randomness injected into the response.

            Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0
            for analytical / multiple choice, and closer to 1.0 for creative and
            generative tasks.

            Note that even with temperature of 0.0, the results will not be fully
            deterministic.
        stopSequences:
          type: array
          items:
            type: string
          description: |-
            Sequences that will cause the model to stop generating.

            Our models stop on "\n\nHuman:", and may include additional built-in stop
            sequences in the future. By providing the `stop_sequences` parameter, you may
            include additional strings that will cause the model to stop generating.
        topK:
          type: integer
          format: int32
          description: |-
            Only sample from the top K options for each subsequent token.

            Used to remove "long tail" low probability responses. Learn more technical details here:
            https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277

            Recommended for advanced use cases only. You usually only need to use temperature.
        topP:
          type: number
          format: float
          description: |-
            Use nucleus sampling.

            In nucleus sampling, we compute the cumulative distribution over all the
            options for each subsequent token in decreasing probability order and cut it
            off once it reaches a particular probability specified by `top_p`. You should
            either alter temperature or `top_p`, but not both.

            Recommended for advanced use cases only. You usually only need to use temperature.
        stream:
          type: boolean
          nullable: true
          description: |-
            Whether to stream the response.

            - When true, the response body is an SSE stream of `event: completion\ndata: JSON` where
            each JSON blob is a `CompletionResponse`.
            - When false, the response body is a single JSON-encoded `CompletionResponse`.
        logprobs:
          type: integer
          format: uint8
          description: |-
            Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
            The maximum value for logprobs is 5.
            For more details, see Fireworks API docs https://docs.fireworks.ai/api-reference/post-completions
    CompletionRequestApiVersion:
      type: number
      enum:
        - 1
        - 2
    CompletionResponse:
      type: object
      properties:
        completion:
          type: string
          description: |-
            Completion is the full completion string. This field
            is only present when using the `/.api/completions/code` or when
            using `api-version=1` or older with `/.api/completions/stream`.

            In the V2 API for streaming responses, the `deltaText` property is used
            instead.
        deltaText:
          type: string
          description: |-
            DeltaText is the incremental text that was added to the prompt.
            This field is only present in streaming responses to `/.api/completions/stream`.
        stopReason:
          type: string
          description: |-
            The reason the model stopped generating tokens. The exact format
            of this field is defined by the model provider.

            For OpenAI models, the following stop reasons are defined:
            - `stop`: the model hit a natural stop point or a provided stop sequence.
            - `length`: the maximum number of tokens specified in the request was reached.
            - `content_filter`: content was omitted due content filters.

            For Anthropic models, the following stop reasons are defined:
            - "stop_sequence": we reached a stop sequence either provided via the
            stop_sequences parameter, or a stop sequence built into the model.
            - "max_tokens": we exceeded max_tokens_to_sample or the model's maximum limit.
        logprobs:
          type: object
          allOf:
            - $ref: '#/components/schemas/Logprobs'
          nullable: true
    ContextWindow:
      type: object
      required:
        - maxInputTokens
        - maxOutputTokens
      properties:
        maxInputTokens:
          type: integer
          format: int32
        maxOutputTokens:
          type: integer
          format: int32
    DefaultModels:
      type: object
      required:
        - chat
        - fastChat
        - codeCompletion
      properties:
        chat:
          type: string
        fastChat:
          type: string
        codeCompletion:
          type: string
    Logprobs:
      type: object
      required:
        - tokens
        - token_logprobs
        - top_logprobs
        - text_offset
      properties:
        tokens:
          type: array
          items:
            type: string
        token_logprobs:
          type: array
          items:
            type: number
            format: float
        top_logprobs:
          type: array
          items:
            type: object
            additionalProperties:
              type: number
              format: float
        text_offset:
          type: array
          items:
            type: integer
            format: int32
    Model:
      type: object
      required:
        - modelRef
        - displayName
        - modelName
        - capabilities
        - category
        - status
        - tier
        - contextWindow
      properties:
        modelRef:
          type: string
        displayName:
          type: string
        modelName:
          type: string
        capabilities:
          type: array
          items:
            $ref: '#/components/schemas/ModelCapability'
        category:
          $ref: '#/components/schemas/ModelCategory'
        status:
          $ref: '#/components/schemas/ModelStatus'
        tier:
          $ref: '#/components/schemas/ModelTier'
        contextWindow:
          $ref: '#/components/schemas/ContextWindow'
    ModelCapability:
      type: string
      enum:
        - autocomplete
        - chat
    ModelCatalog:
      type: object
      required:
        - schemaVersion
        - revision
        - providers
        - models
        - defaultModels
      properties:
        schemaVersion:
          type: string
        revision:
          type: string
        providers:
          type: array
          items:
            $ref: '#/components/schemas/Provider'
        models:
          type: array
          items:
            $ref: '#/components/schemas/Model'
        defaultModels:
          $ref: '#/components/schemas/DefaultModels'
    ModelCategory:
      type: string
      enum:
        - accuracy
        - balanced
        - speed
    ModelStatus:
      type: string
      enum:
        - experimental
        - beta
        - stable
        - deprecated
    ModelTier:
      type: string
      enum:
        - free
        - pro
        - enterprise
    Provider:
      type: object
      required:
        - id
        - displayName
      properties:
        id:
          type: string
        displayName:
          type: string
    Versions:
      type: string
      enum:
        - V5_5
        - V5_6
        - V5_7
        - V5_8
        - Latest
